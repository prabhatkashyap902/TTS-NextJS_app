{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéôÔ∏è Kokoro TTS API Server v5\n",
        "\n",
        "This notebook runs Kokoro TTS as an API for your NeuralTTS app.\n",
        "\n",
        "**How to use:**\n",
        "1. Go to Runtime ‚Üí Change runtime type ‚Üí Select **T4 GPU**\n",
        "2. Run all cells (Ctrl+F9)\n",
        "3. Copy the ngrok URL printed at the end\n",
        "4. Paste it in your Cloud TTS page\n",
        "\n",
        "**v5 Features:**\n",
        "- Uses Kokoro's native text processing (same as local-tts)\n",
        "- Real-time progress from internal chunk generation\n",
        "- Model cached in Google Drive"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Install Dependencies"
      ],
      "metadata": {
        "id": "deps_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q kokoro>=0.9.4 flask flask-cors pyngrok soundfile numpy torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Mount Google Drive & Setup Cache"
      ],
      "metadata": {
        "id": "drive_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cache directory in Google Drive\n",
        "CACHE_DIR = '/content/drive/MyDrive/kokoro_cache'\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Set HuggingFace cache to our Drive folder\n",
        "os.environ['HF_HOME'] = CACHE_DIR\n",
        "os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR\n",
        "os.environ['HF_HUB_CACHE'] = os.path.join(CACHE_DIR, 'hub')\n",
        "\n",
        "print(f\"‚úÖ Cache directory: {CACHE_DIR}\")\n",
        "print(f\"   Existing files: {os.listdir(CACHE_DIR) if os.path.exists(CACHE_DIR) else 'Empty'}\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Load Kokoro Model (Uses cache if available)"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kokoro import KPipeline\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Loading Kokoro model...\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Initialize pipeline - will use cache from Google Drive\n",
        "pipeline = KPipeline(lang_code='a')  # 'a' = American English, 'b' = British\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Setup ngrok (Get your free token from ngrok.com)"
      ],
      "metadata": {
        "id": "ngrok_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enter your ngrok authtoken (get free at https://ngrok.com)\n",
        "NGROK_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ ngrok token set!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No ngrok token - using free tier (limited)\")\n",
        "    print(\"   Get a free token at: https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ],
      "metadata": {
        "id": "ngrok_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Start API Server (v5 - Native Kokoro Processing)"
      ],
      "metadata": {
        "id": "api_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, send_file, jsonify, Response\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import io\n",
        "import torch\n",
        "import json\n",
        "import base64\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# All Kokoro voices\n",
        "VOICES = {\n",
        "    \"af_bella\": {\"name\": \"Bella\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_nicole\": {\"name\": \"Nicole\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_sarah\": {\"name\": \"Sarah\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_sky\": {\"name\": \"Sky\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_heart\": {\"name\": \"Heart\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_alloy\": {\"name\": \"Alloy\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_aoede\": {\"name\": \"Aoede\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_jessica\": {\"name\": \"Jessica\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"af_kore\": {\"name\": \"Kore\", \"gender\": \"Female\", \"lang\": \"en-us\"},\n",
        "    \"am_adam\": {\"name\": \"Adam\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_michael\": {\"name\": \"Michael\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_echo\": {\"name\": \"Echo\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_eric\": {\"name\": \"Eric\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_fenrir\": {\"name\": \"Fenrir\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_liam\": {\"name\": \"Liam\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_onyx\": {\"name\": \"Onyx\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"am_puck\": {\"name\": \"Puck\", \"gender\": \"Male\", \"lang\": \"en-us\"},\n",
        "    \"bf_emma\": {\"name\": \"Emma\", \"gender\": \"Female\", \"lang\": \"en-gb\"},\n",
        "    \"bf_isabella\": {\"name\": \"Isabella\", \"gender\": \"Female\", \"lang\": \"en-gb\"},\n",
        "    \"bm_george\": {\"name\": \"George\", \"gender\": \"Male\", \"lang\": \"en-gb\"},\n",
        "    \"bm_lewis\": {\"name\": \"Lewis\", \"gender\": \"Male\", \"lang\": \"en-gb\"},\n",
        "}\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"ok\",\n",
        "        \"model\": \"kokoro\",\n",
        "        \"version\": \"v5\",\n",
        "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "    })\n",
        "\n",
        "@app.route('/api/voices', methods=['GET'])\n",
        "def get_voices():\n",
        "    return jsonify(VOICES)\n",
        "\n",
        "@app.route('/api/tts', methods=['POST'])\n",
        "def generate_tts():\n",
        "    \"\"\"Generate TTS using Kokoro's native chunking with progress streaming.\"\"\"\n",
        "    data = request.json\n",
        "    text = data.get('text', 'Hello world')\n",
        "    voice = data.get('voice', 'af_bella')\n",
        "    speed = float(data.get('speed', 1.0))\n",
        "    stream_progress = data.get('stream', False)\n",
        "    \n",
        "    speed = max(0.5, min(2.0, speed))\n",
        "    \n",
        "    if not stream_progress:\n",
        "        # Non-streaming mode - simple blob response\n",
        "        try:\n",
        "            print(f\"üéôÔ∏è Generating: voice={voice}, speed={speed}, chars={len(text)}\")\n",
        "            \n",
        "            # Use Kokoro's native generator - NO manual text splitting!\n",
        "            generator = pipeline(text, voice=voice, speed=speed)\n",
        "            audio_chunks = []\n",
        "            for i, (gs, ps, audio) in enumerate(generator):\n",
        "                audio_chunks.append(audio)\n",
        "                print(f\"   Chunk {i+1} generated\")\n",
        "            \n",
        "            final_audio = torch.cat(audio_chunks).numpy()\n",
        "            \n",
        "            buffer = io.BytesIO()\n",
        "            sf.write(buffer, final_audio, 24000, format='WAV')\n",
        "            buffer.seek(0)\n",
        "            \n",
        "            print(f\"‚úÖ Generated {len(final_audio)/24000:.2f}s of audio\")\n",
        "            return send_file(buffer, mimetype='audio/wav', as_attachment=True, download_name='tts_output.wav')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            return jsonify({\"error\": str(e)}), 500\n",
        "    \n",
        "    # Streaming mode with progress from Kokoro's native chunks\n",
        "    def generate_with_progress():\n",
        "        try:\n",
        "            print(f\"üéôÔ∏è [Stream] Generating: voice={voice}, speed={speed}, chars={len(text)}\")\n",
        "            \n",
        "            # Convert generator to list to know total chunks\n",
        "            # Note: We need to consume once to count, then regenerate\n",
        "            # First, estimate chunks based on text length (~100 chars per chunk)\n",
        "            estimated_chunks = max(1, len(text) // 100)\n",
        "            \n",
        "            yield f\"data: {json.dumps({'type': 'progress', 'current': 0, 'total': estimated_chunks, 'percent': 0, 'status': 'Starting...'})}\\n\\n\"\n",
        "            \n",
        "            # Generate using Kokoro's native pipeline (handles text normalization properly)\n",
        "            generator = pipeline(text, voice=voice, speed=speed)\n",
        "            audio_chunks = []\n",
        "            chunk_count = 0\n",
        "            \n",
        "            for i, (gs, ps, audio) in enumerate(generator):\n",
        "                audio_chunks.append(audio)\n",
        "                chunk_count = i + 1\n",
        "                \n",
        "                # Update estimate based on actual progress\n",
        "                percent = min(95, int((chunk_count / max(estimated_chunks, chunk_count)) * 100))\n",
        "                \n",
        "                progress_data = {\n",
        "                    'type': 'progress',\n",
        "                    'current': chunk_count,\n",
        "                    'total': max(estimated_chunks, chunk_count),\n",
        "                    'percent': percent,\n",
        "                    'status': f'Processing chunk {chunk_count}...'\n",
        "                }\n",
        "                yield f\"data: {json.dumps(progress_data)}\\n\\n\"\n",
        "                print(f\"   Chunk {chunk_count} ({percent}%)\")\n",
        "            \n",
        "            if audio_chunks:\n",
        "                final_audio = torch.cat(audio_chunks).numpy()\n",
        "                \n",
        "                buffer = io.BytesIO()\n",
        "                sf.write(buffer, final_audio, 24000, format='WAV')\n",
        "                buffer.seek(0)\n",
        "                audio_b64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "                \n",
        "                duration = len(final_audio) / 24000\n",
        "                print(f\"‚úÖ Generated {duration:.2f}s of audio ({chunk_count} native chunks)\")\n",
        "                \n",
        "                yield f\"data: {json.dumps({'type': 'complete', 'audio': audio_b64, 'duration': duration, 'chunks': chunk_count})}\\n\\n\"\n",
        "            else:\n",
        "                yield f\"data: {json.dumps({'type': 'error', 'message': 'No audio generated'})}\\n\\n\"\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n",
        "    \n",
        "    return Response(\n",
        "        generate_with_progress(),\n",
        "        mimetype='text/event-stream',\n",
        "        headers={\n",
        "            'Cache-Control': 'no-cache',\n",
        "            'Connection': 'keep-alive',\n",
        "            'X-Accel-Buffering': 'no'\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ API SERVER IS RUNNING! (v5 - Native Kokoro)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìã Copy this URL to your Cloud TTS page:\\n\")\n",
        "print(f\"   {public_url}\")\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"\\n‚ÑπÔ∏è  v5 uses Kokoro's native text processing\")\n",
        "print(\"   Same audio quality as local-tts!\")\n",
        "print(\"\\n‚è∞ Keep this notebook running while using Cloud TTS\")\n",
        "print(\"   Free tier: ~1-2 hours of GPU time per day\\n\")\n",
        "\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
