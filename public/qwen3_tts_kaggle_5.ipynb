{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéôÔ∏è Qwen3-TTS for Kaggle (Dual T4 - 2x Speed!)\n",
        "\n",
        "**Uses BOTH T4 GPUs** - processes 2 chunks in parallel for ~2x faster generation!\n",
        "\n",
        "**Before running:**\n",
        "1. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2** (REQUIRED!)\n",
        "2. Settings ‚Üí Internet ‚Üí **ON**\n",
        "3. Run cells with **Shift+Enter** (one by one!)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q qwen-tts flask flask-cors pyngrok soundfile numpy"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from qwen_tts import Qwen3TTSModel\n",
        "\n",
        "print(\"üîÑ Loading Qwen3-TTS on BOTH GPUs...\")\n",
        "print(f\"   GPU 0: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"   GPU 1: {torch.cuda.get_device_name(1)}\")\n",
        "\n",
        "# Load model on GPU 0\n",
        "print(\"\\nüì¶ Loading model on GPU 0...\")\n",
        "model_0 = Qwen3TTSModel.from_pretrained(\n",
        "    \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n",
        "    device_map=\"cuda:0\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "print(\"‚úÖ GPU 0 ready!\")\n",
        "\n",
        "# Load model on GPU 1\n",
        "print(\"\\nüì¶ Loading model on GPU 1...\")\n",
        "model_1 = Qwen3TTSModel.from_pretrained(\n",
        "    \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n",
        "    device_map=\"cuda:1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "print(\"‚úÖ GPU 1 ready!\")\n",
        "\n",
        "print(\"\\nüöÄ Both GPUs loaded and ready for parallel processing!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PASTE YOUR NGROK TOKEN HERE!\n",
        "NGROK_TOKEN = \"\"  # <-- Get from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "from pyngrok import ngrok\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ ngrok token set!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Enter your ngrok token above!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, send_file, jsonify, Response\n",
        "from flask_cors import CORS\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import io, base64, tempfile, os, json, re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "CHUNK_SIZE = 1000  # 1K characters per chunk\n",
        "\n",
        "# Thread-safe locks for each GPU\n",
        "gpu_locks = [threading.Lock(), threading.Lock()]\n",
        "\n",
        "def split_text(text, max_chars=CHUNK_SIZE):\n",
        "    \"\"\"Split text into chunks at sentence boundaries\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?‡•§])\\s+', text)\n",
        "    chunks = []\n",
        "    current = \"\"\n",
        "    for s in sentences:\n",
        "        if len(current) + len(s) <= max_chars:\n",
        "            current += (\" \" if current else \"\") + s\n",
        "        else:\n",
        "            if current:\n",
        "                chunks.append(current.strip())\n",
        "            if len(s) > max_chars:\n",
        "                words = s.split()\n",
        "                current = \"\"\n",
        "                for word in words:\n",
        "                    if len(current) + len(word) + 1 <= max_chars:\n",
        "                        current += (\" \" if current else \"\") + word\n",
        "                    else:\n",
        "                        if current:\n",
        "                            chunks.append(current.strip())\n",
        "                        current = word\n",
        "            else:\n",
        "                current = s\n",
        "    if current:\n",
        "        chunks.append(current.strip())\n",
        "    return chunks if chunks else [text]\n",
        "\n",
        "def generate_chunk(chunk_idx, chunk_text, language, ref_path, ref_text, gpu_id):\n",
        "    \"\"\"Generate audio for a single chunk on specified GPU\"\"\"\n",
        "    model = model_0 if gpu_id == 0 else model_1\n",
        "    \n",
        "    with gpu_locks[gpu_id]:\n",
        "        try:\n",
        "            if ref_text and ref_text.strip():\n",
        "                wavs, sr = model.generate_voice_clone(text=chunk_text, language=language, ref_audio=ref_path, ref_text=ref_text)\n",
        "            else:\n",
        "                wavs, sr = model.generate_voice_clone(text=chunk_text, language=language, ref_audio=ref_path, x_vector_only_mode=True)\n",
        "            return (chunk_idx, wavs[0], sr, None)\n",
        "        except Exception as e:\n",
        "            return (chunk_idx, None, None, str(e))\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"ok\", \n",
        "        \"model\": \"Qwen3-TTS-1.7B\", \n",
        "        \"gpu_0\": torch.cuda.get_device_name(0),\n",
        "        \"gpu_1\": torch.cuda.get_device_name(1),\n",
        "        \"mode\": \"dual-gpu-parallel\",\n",
        "        \"chunk_size\": CHUNK_SIZE\n",
        "    })\n",
        "\n",
        "@app.route('/api/tts', methods=['POST'])\n",
        "def generate_tts():\n",
        "    data = request.json\n",
        "    text = data.get('text', 'Hello')\n",
        "    language = data.get('language', 'English')\n",
        "    ref_audio_b64 = data.get('ref_audio')\n",
        "    ref_text = data.get('ref_text', '')\n",
        "    stream = data.get('stream', False)\n",
        "\n",
        "    def generate_with_progress():\n",
        "        try:\n",
        "            if not ref_audio_b64:\n",
        "                yield f\"data: {json.dumps({'type': 'error', 'message': 'Reference audio is required'})}\\n\\n\"\n",
        "                return\n",
        "            \n",
        "            chunks = split_text(text, CHUNK_SIZE)\n",
        "            total_chunks = len(chunks)\n",
        "            \n",
        "            print(f\"üéôÔ∏è Dual-GPU: {len(text)} chars ‚Üí {total_chunks} chunks\")\n",
        "            \n",
        "            yield f\"data: {json.dumps({'type': 'progress', 'current': 0, 'total': total_chunks, 'percent': 0, 'status': f'Preparing {total_chunks} chunks (2 GPUs)...'})}\\n\\n\"\n",
        "            \n",
        "            # Decode reference audio\n",
        "            audio_bytes = base64.b64decode(ref_audio_b64)\n",
        "            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:\n",
        "                f.write(audio_bytes)\n",
        "                ref_path = f.name\n",
        "            \n",
        "            yield f\"data: {json.dumps({'type': 'progress', 'current': 0, 'total': total_chunks, 'percent': 5, 'status': 'Reference audio loaded'})}\\n\\n\"\n",
        "            \n",
        "            # Process chunks in pairs (parallel on 2 GPUs)\n",
        "            results = {}  # Store results by chunk index\n",
        "            sample_rate = None\n",
        "            completed = 0\n",
        "            \n",
        "            with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "                futures = {}\n",
        "                \n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    gpu_id = i % 2  # Alternate between GPU 0 and GPU 1\n",
        "                    future = executor.submit(generate_chunk, i, chunk, language, ref_path, ref_text, gpu_id)\n",
        "                    futures[future] = i\n",
        "                \n",
        "                for future in as_completed(futures):\n",
        "                    chunk_idx, audio, sr, error = future.result()\n",
        "                    \n",
        "                    if error:\n",
        "                        print(f\"‚ùå Chunk {chunk_idx+1} failed: {error}\")\n",
        "                        yield f\"data: {json.dumps({'type': 'error', 'message': f'Chunk {chunk_idx+1} failed: {error}'})}\\n\\n\"\n",
        "                        os.unlink(ref_path)\n",
        "                        return\n",
        "                    \n",
        "                    results[chunk_idx] = audio\n",
        "                    if sample_rate is None:\n",
        "                        sample_rate = sr\n",
        "                    \n",
        "                    completed += 1\n",
        "                    pct = int((completed / total_chunks) * 90) + 5\n",
        "                    gpu_used = chunk_idx % 2\n",
        "                    yield f\"data: {json.dumps({'type': 'progress', 'current': completed, 'total': total_chunks, 'percent': pct, 'status': f'Chunk {chunk_idx+1} done (GPU{gpu_used}) - {completed}/{total_chunks}'})}\\n\\n\"\n",
        "            \n",
        "            os.unlink(ref_path)\n",
        "            \n",
        "            yield f\"data: {json.dumps({'type': 'progress', 'current': total_chunks, 'total': total_chunks, 'percent': 95, 'status': 'Merging audio in order...'})}\\n\\n\"\n",
        "            \n",
        "            # Concatenate in ORDER (0, 1, 2, 3, ...)\n",
        "            ordered_audio = [results[i] for i in range(total_chunks)]\n",
        "            final_audio = np.concatenate(ordered_audio)\n",
        "            \n",
        "            buffer = io.BytesIO()\n",
        "            sf.write(buffer, final_audio, sample_rate, format='WAV')\n",
        "            buffer.seek(0)\n",
        "            audio_b64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "            \n",
        "            duration = len(final_audio) / sample_rate\n",
        "            print(f\"‚úÖ Generated {duration:.1f}s from {total_chunks} chunks (dual-GPU)\")\n",
        "            \n",
        "            yield f\"data: {json.dumps({'type': 'complete', 'audio': audio_b64, 'duration': round(duration, 1), 'chunks': total_chunks})}\\n\\n\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            import traceback; traceback.print_exc()\n",
        "            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n",
        "\n",
        "    if stream:\n",
        "        return Response(generate_with_progress(), mimetype='text/event-stream', headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Use stream=true\"}), 400\n",
        "\n",
        "@app.route('/api/clone', methods=['POST'])\n",
        "def voice_clone():\n",
        "    return generate_tts()\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"üöÄ QWEN3-TTS URL: {public_url}\")\n",
        "print(f\"   Mode: DUAL-GPU (2x speed!)\")\n",
        "print(f\"   Chunk size: {CHUNK_SIZE} chars\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "app.run(port=5000)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
