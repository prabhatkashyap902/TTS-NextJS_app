{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéôÔ∏è Kokoro TTS for Kaggle v3\n",
        "\n",
        "**v3: Uses Localtunnel instead of ngrok (FREE, NO LIMITS!)**\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
        "2. Enable Internet: Settings ‚Üí Internet ‚Üí ON\n",
        "3. ‚ö†Ô∏è Verify phone if needed for GPU access\n",
        "4. Run cells one by one (Shift+Enter)\n",
        "5. Copy the localtunnel URL"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kokoro>=0.9.4 flask flask-cors soundfile numpy torch\n",
        "!npm install -g localtunnel"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kokoro import KPipeline\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Loading Kokoro model...\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "pipeline = KPipeline(lang_code='a')\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, send_file, jsonify, Response\n",
        "from flask_cors import CORS\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import io, json, base64\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "# Explicit CORS headers for all responses\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type')\n",
        "    response.headers.add('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')\n",
        "    return response\n",
        "\n",
        "@app.route('/health', methods=['GET', 'OPTIONS'])\n",
        "def health():\n",
        "    if request.method == 'OPTIONS':\n",
        "        return '', 200\n",
        "    return jsonify({\"status\": \"ok\", \"model\": \"kokoro\", \"version\": \"v3-localtunnel\", \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"})\n",
        "\n",
        "@app.route('/api/voices', methods=['GET', 'OPTIONS'])\n",
        "def get_voices():\n",
        "    if request.method == 'OPTIONS':\n",
        "        return '', 200\n",
        "    VOICES = {\n",
        "        \"af_bella\": \"Bella\", \"af_nicole\": \"Nicole\", \"af_sarah\": \"Sarah\",\n",
        "        \"af_sky\": \"Sky\", \"am_adam\": \"Adam\", \"am_michael\": \"Michael\",\n",
        "        \"bf_emma\": \"Emma\", \"bm_george\": \"George\"\n",
        "    }\n",
        "    return jsonify(VOICES)\n",
        "\n",
        "@app.route('/api/tts', methods=['POST', 'OPTIONS'])\n",
        "def generate_tts():\n",
        "    if request.method == 'OPTIONS':\n",
        "        return '', 200\n",
        "    \n",
        "    data = request.json\n",
        "    text = data.get('text', 'Hello')\n",
        "    voice = data.get('voice', 'af_bella')\n",
        "    speed = float(data.get('speed', 1.0))\n",
        "    stream = data.get('stream', False)\n",
        "    \n",
        "    if not stream:\n",
        "        try:\n",
        "            print(f\"üéôÔ∏è Gen: voice={voice}, speed={speed}, chars={len(text)}\")\n",
        "            generator = pipeline(text, voice=voice, speed=speed)\n",
        "            audio_chunks = [audio for _, _, audio in generator]\n",
        "            final = torch.cat(audio_chunks).numpy()\n",
        "            \n",
        "            buffer = io.BytesIO()\n",
        "            sf.write(buffer, final, 24000, format='WAV')\n",
        "            buffer.seek(0)\n",
        "            print(f\"‚úÖ Generated {len(final)/24000:.2f}s\")\n",
        "            return send_file(buffer, mimetype='audio/wav')\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": str(e)}), 500\n",
        "    \n",
        "    def generate_with_progress():\n",
        "        try:\n",
        "            print(f\"üéôÔ∏è [Stream] voice={voice}, speed={speed}\")\n",
        "            estimated = max(1, len(text) // 100)\n",
        "            yield f\"data: {json.dumps({'type': 'progress', 'current': 0, 'total': estimated, 'percent': 0, 'status': 'Starting...'})}\\n\\n\"\n",
        "            \n",
        "            generator = pipeline(text, voice=voice, speed=speed)\n",
        "            audio_chunks = []\n",
        "            count = 0\n",
        "            \n",
        "            for _, _, audio in generator:\n",
        "                audio_chunks.append(audio)\n",
        "                count += 1\n",
        "                pct = min(95, int((count / max(estimated, count)) * 100))\n",
        "                yield f\"data: {json.dumps({'type': 'progress', 'current': count, 'total': max(estimated, count), 'percent': pct, 'status': f'Chunk {count}...'})}\\n\\n\"\n",
        "            \n",
        "            final = torch.cat(audio_chunks).numpy()\n",
        "            buffer = io.BytesIO()\n",
        "            sf.write(buffer, final, 24000, format='WAV')\n",
        "            buffer.seek(0)\n",
        "            audio_b64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "            \n",
        "            yield f\"data: {json.dumps({'type': 'complete', 'audio': audio_b64, 'duration': len(final)/24000})}\\n\\n\"\n",
        "        except Exception as e:\n",
        "            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n",
        "    \n",
        "    return Response(generate_with_progress(), mimetype='text/event-stream', headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})\n",
        "\n",
        "# Start localtunnel in background\n",
        "def start_tunnel():\n",
        "    time.sleep(2)  # Wait for Flask to start\n",
        "    process = subprocess.Popen(\n",
        "        ['lt', '--port', '5000'],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        if 'your url is' in line.lower():\n",
        "            url = line.strip().split()[-1]\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"üöÄ KOKORO TTS URL: {url}\")\n",
        "            print(\"   Version: v3 (localtunnel - FREE, NO LIMITS!)\")\n",
        "            print(\"=\"*50)\n",
        "            print(\"\\n‚ö†Ô∏è  First time: Click the link and enter your IP at the page\")\n",
        "            print(\"   Then the API will work!\\n\")\n",
        "            break\n",
        "\n",
        "tunnel_thread = threading.Thread(target=start_tunnel, daemon=True)\n",
        "tunnel_thread.start()\n",
        "\n",
        "print(\"Starting Flask server...\")\n",
        "app.run(port=5000, use_reloader=False)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
